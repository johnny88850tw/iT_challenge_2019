{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('./dataset/Google_Stock_Price_Train.csv').values\n",
    "test_data = pd.read_csv('./dataset/Google_Stock_Price_Test.csv').values\n",
    "\n",
    "# parameter\n",
    "input_days = 10\n",
    "local_norm_flag = True\n",
    "epochs = 1000\n",
    "batch_size = 500\n",
    "period = 100\n",
    "offset = 0.5\n",
    "RADAM = False\n",
    "if RADAM:\n",
    "    from keras_radam import RAdam\n",
    "    from keras_lookahead import Lookahead\n",
    "    optimizer = Lookahead(RAdam())\n",
    "else:\n",
    "    optimizer = 'sgd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix data string to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1258, 5)\n",
      "(20, 5)\n"
     ]
    }
   ],
   "source": [
    "# the data[4:6] must be fix\n",
    "def str2float(data):\n",
    "    length = len(data)\n",
    "    for i in range(length):\n",
    "        try:\n",
    "            data[i] = data[i].replace(',', '')\n",
    "        except AttributeError:\n",
    "            data[i] = data[i]\n",
    "    return np.asarray(data, dtype=np.float)\n",
    "    \n",
    "# fix all data in dataset\n",
    "def fixStr2Float(dataset):\n",
    "    shape = dataset.shape\n",
    "    dataset_t = np.zeros((0, shape[-1]), np.float)\n",
    "    for i, data in enumerate(dataset):\n",
    "        dataset_t = np.append(dataset_t, np.expand_dims(str2float(data), axis=0), axis=0)\n",
    "    return dataset_t\n",
    "\n",
    "# trainsform\n",
    "train_data_t = fixStr2Float(train_data[:, 1:])\n",
    "test_data_t = fixStr2Float(test_data[:, 1:])\n",
    "\n",
    "print(train_data_t.shape)\n",
    "print(test_data_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My own MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxScaler:\n",
    "    __min = 0.\n",
    "    __max = 1.\n",
    "    __range = 1.\n",
    "    __feature_range = (0, 1)\n",
    "    __scale = 1.\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def getScalerData(self, dataset, offset=0.1, feature_range=(0, 1)):\n",
    "        data_max = np.max(dataset)\n",
    "        data_min = np.min(dataset)\n",
    "        self.__range = (data_max - data_min) * (1 + offset)\n",
    "        self.__min = data_max - self.__range\n",
    "        self.__max = data_min + self.__range\n",
    "        self.__feature_range = feature_range\n",
    "        self.__scale = (feature_range[1] - feature_range[0]) / self.__range\n",
    "        return self.getTransformData(dataset)\n",
    "    def getTransformData(self, dataset):\n",
    "        return (dataset - self.__min) * self.__scale + self.__feature_range[0]\n",
    "    def getInverseData(self, scalerDataset):\n",
    "        return (scalerDataset - self.__feature_range[0]) / self.__scale + self.__min\n",
    "    def getParameter(self):\n",
    "        return self.__min, self.__max, self.__range, self.__feature_range, self.__scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataset_global(dataset, day_in=60, day_out=1):\n",
    "    count = len(dataset)\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(day_in, count - day_out + 1):\n",
    "        x.append(dataset[i-day_in:i, :])\n",
    "        y.append(dataset[i:i+day_out, :])\n",
    "    return np.asarray(x), np.asarray(y)\n",
    "\n",
    "def genQuteChange(dataset):\n",
    "    return (dataset[1:] - dataset[:-1]) / dataset[:-1]\n",
    "\n",
    "def createDataset_local(dataset, day_in=60, day_out=1, offset=0.1):\n",
    "    sc = MinMaxScaler()\n",
    "    count = len(dataset)\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(day_in, count - day_out + 1):\n",
    "        x.append(sc.getScalerData(dataset[i-day_in:i, :], offset=offset))\n",
    "        y.append(sc.getTransformData(dataset[i:i+day_out, :]))\n",
    "    return np.asarray(x), np.asarray(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training dataset and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1248, 10, 2) (1248, 1, 1)\n",
      "(30, 10, 2) (30, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# append to a big dataset total\n",
    "dataset = np.append(train_data_t, test_data_t, axis=0)\n",
    "test_count = len(test_data_t)\n",
    "\n",
    "# Split dataset to Volume and Open\n",
    "open_data = dataset[:, :1]\n",
    "volume_data = dataset[:, -1:]\n",
    "\n",
    "# use global norm to volume data (with offset)\n",
    "volume_sc = MinMaxScaler()\n",
    "volume_norm = volume_sc.getScalerData(volume_data, offset=0.05, feature_range=(0, 1))\n",
    "\n",
    "# create dataset\n",
    "volume_dataset = createDataset_global(volume_norm, day_in=input_days)\n",
    "if local_norm_flag:\n",
    "    open_dataset = createDataset_local(dataset[:, :1], day_in=input_days, offset=offset)\n",
    "else:\n",
    "    open_sc = MinMaxScaler()\n",
    "    open_qute = genQuteChange(dataset[:, :1])\n",
    "    open_norm = open_sc.getScalerData(open_qute, offset=0, feature_range=(0, 1))\n",
    "    open_dataset = createDataset_global(open_norm, day_in=input_days)\n",
    "    # fix volume dataset\n",
    "    volume_dataset = (volume_dataset[0][1:], volume_dataset[1][1:])\n",
    "\n",
    "# create total dataset\n",
    "dataset_x = np.append(open_dataset[0], volume_dataset[0], axis=-1)\n",
    "dataset_y = np.append(open_dataset[1], volume_dataset[1], axis=-1)\n",
    "\n",
    "# split to train and test dataset\n",
    "train_x = dataset_x[:-test_count]\n",
    "train_y = dataset_y[:-test_count, :, :1]\n",
    "test_x = dataset_x[-test_count-input_days:]\n",
    "test_y = dataset_y[-test_count-input_days:, :, :1]\n",
    "\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# show volume data detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33.66417403] [-0.99747065]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "count = len(volume_data)\n",
    "min_ = 1\n",
    "max_ = 0\n",
    "stat = []\n",
    "sc = MinMaxScaler()\n",
    "volume_data_temp = sc.getScalerData(volume_data, offset=0, feature_range=(0, 35))\n",
    "for i in range(1, count):\n",
    "    temp = (volume_data[i] - volume_data[i-1]) / volume_data[i-1]\n",
    "    stat.append(temp[0])\n",
    "    if temp > max_:\n",
    "        max_ = temp\n",
    "    elif temp < min_:\n",
    "        min_ = temp\n",
    "print(max_, min_)\n",
    "\n",
    "# Visualising the results\n",
    "plt.plot(volume_data_temp, color = 'blue', label = 'value')\n",
    "plt.plot(stat, color = 'red', label = 'amplitude')\n",
    "plt.title('Prediction')\n",
    "plt.xlabel('Time(days)')\n",
    "plt.ylabel('Real')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DVLAB\\AppData\\anaconda3\\envs\\dlib\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\DVLAB\\AppData\\anaconda3\\envs\\dlib\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\DVLAB\\AppData\\anaconda3\\envs\\dlib\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 10, 50)            10600     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 50)            20200     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               64128     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 40)                5160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 100,129\n",
      "Trainable params: 100,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LSTM Training\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(units = 50, return_sequences = True, input_shape = (input_days, 2), dropout=0.2))\n",
    "model.add(LSTM(units = 50, return_sequences = True, dropout=0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 128, activation='relu'))\n",
    "model.add(Dense(units = 40, activation='relu'))\n",
    "model.add(Dense(units = 1))\n",
    "model.compile(optimizer = optimizer, loss = 'mean_squared_error')\n",
    "model.summary()\n",
    "plot_model(model, 'model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model with `Open` data and `Volume` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1248 samples, validate on 30 samples\n",
      "WARNING:tensorflow:From C:\\Users\\DVLAB\\AppData\\anaconda3\\envs\\dlib\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1000\n",
      "1248/1248 [==============================] - 2s 2ms/sample - loss: 0.6088 - val_loss: 1.1964\n",
      "Epoch 2/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.4872 - val_loss: 1.0721\n",
      "Epoch 3/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.4000 - val_loss: 0.9719\n",
      "Epoch 4/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.3310 - val_loss: 0.8853\n",
      "Epoch 5/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.2753 - val_loss: 0.8148\n",
      "Epoch 6/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.2325 - val_loss: 0.7576\n",
      "Epoch 7/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.2015 - val_loss: 0.7129\n",
      "Epoch 8/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1763 - val_loss: 0.6772\n",
      "Epoch 9/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1596 - val_loss: 0.6495\n",
      "Epoch 10/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1482 - val_loss: 0.6287\n",
      "Epoch 11/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1400 - val_loss: 0.6126\n",
      "Epoch 12/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1352 - val_loss: 0.6001\n",
      "Epoch 13/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1316 - val_loss: 0.5909\n",
      "Epoch 14/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1295 - val_loss: 0.5830\n",
      "Epoch 15/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1276 - val_loss: 0.5779\n",
      "Epoch 16/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1275 - val_loss: 0.5736\n",
      "Epoch 17/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1266 - val_loss: 0.5704\n",
      "Epoch 18/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1253 - val_loss: 0.5680\n",
      "Epoch 19/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1244 - val_loss: 0.5658\n",
      "Epoch 20/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1247 - val_loss: 0.5634\n",
      "Epoch 21/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1257 - val_loss: 0.5625\n",
      "Epoch 22/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1249 - val_loss: 0.5624\n",
      "Epoch 23/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1252 - val_loss: 0.5614\n",
      "Epoch 24/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1247 - val_loss: 0.5609\n",
      "Epoch 25/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1254 - val_loss: 0.5598\n",
      "Epoch 26/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1247 - val_loss: 0.5600\n",
      "Epoch 27/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1237 - val_loss: 0.5598\n",
      "Epoch 28/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1247 - val_loss: 0.5602\n",
      "Epoch 29/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1244 - val_loss: 0.5602\n",
      "Epoch 30/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1249 - val_loss: 0.5601\n",
      "Epoch 31/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1239 - val_loss: 0.5598\n",
      "Epoch 32/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1236 - val_loss: 0.5599\n",
      "Epoch 33/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1244 - val_loss: 0.5600\n",
      "Epoch 34/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1245 - val_loss: 0.5597\n",
      "Epoch 35/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1239 - val_loss: 0.5596\n",
      "Epoch 36/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1242 - val_loss: 0.5598\n",
      "Epoch 37/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1238 - val_loss: 0.5595\n",
      "Epoch 38/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1229 - val_loss: 0.5593\n",
      "Epoch 39/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1232 - val_loss: 0.5597\n",
      "Epoch 40/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1247 - val_loss: 0.5598\n",
      "Epoch 41/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1242 - val_loss: 0.5596\n",
      "Epoch 42/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1227 - val_loss: 0.5588\n",
      "Epoch 43/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1231 - val_loss: 0.5580\n",
      "Epoch 44/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1234 - val_loss: 0.5578\n",
      "Epoch 45/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1228 - val_loss: 0.5580\n",
      "Epoch 46/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1232 - val_loss: 0.5586\n",
      "Epoch 47/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1231 - val_loss: 0.5587\n",
      "Epoch 48/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1231 - val_loss: 0.5595\n",
      "Epoch 49/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1234 - val_loss: 0.5587\n",
      "Epoch 50/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1235 - val_loss: 0.5582\n",
      "Epoch 51/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1228 - val_loss: 0.5580\n",
      "Epoch 52/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1232 - val_loss: 0.5583\n",
      "Epoch 53/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1239 - val_loss: 0.5583\n",
      "Epoch 54/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1225 - val_loss: 0.5581\n",
      "Epoch 55/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1234 - val_loss: 0.5588\n",
      "Epoch 56/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1230 - val_loss: 0.5576\n",
      "Epoch 57/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1220 - val_loss: 0.5576\n",
      "Epoch 58/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1218 - val_loss: 0.5579\n",
      "Epoch 59/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1230 - val_loss: 0.5576\n",
      "Epoch 60/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1215 - val_loss: 0.5576\n",
      "Epoch 61/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1228 - val_loss: 0.5574\n",
      "Epoch 62/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1223 - val_loss: 0.5576\n",
      "Epoch 63/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1221 - val_loss: 0.5575\n",
      "Epoch 64/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1225 - val_loss: 0.5572\n",
      "Epoch 65/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1222 - val_loss: 0.5573\n",
      "Epoch 66/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1226 - val_loss: 0.5572\n",
      "Epoch 67/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1219 - val_loss: 0.5570\n",
      "Epoch 68/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1216 - val_loss: 0.5569\n",
      "Epoch 69/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1222 - val_loss: 0.5569\n",
      "Epoch 70/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1221 - val_loss: 0.5580\n",
      "Epoch 71/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1221 - val_loss: 0.5582\n",
      "Epoch 72/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1218 - val_loss: 0.5589\n",
      "Epoch 73/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1224 - val_loss: 0.5585\n",
      "Epoch 74/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1219 - val_loss: 0.5581\n",
      "Epoch 75/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1213 - val_loss: 0.5582\n",
      "Epoch 76/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1220 - val_loss: 0.5575\n",
      "Epoch 77/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1212 - val_loss: 0.5578\n",
      "Epoch 78/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1221 - val_loss: 0.5574\n",
      "Epoch 79/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1213 - val_loss: 0.5576\n",
      "Epoch 80/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1216 - val_loss: 0.5564\n",
      "Epoch 81/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1211 - val_loss: 0.5567\n",
      "Epoch 82/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1215 - val_loss: 0.5570\n",
      "Epoch 83/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1213 - val_loss: 0.5567\n",
      "Epoch 84/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1213 - val_loss: 0.5579\n",
      "Epoch 85/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1216 - val_loss: 0.5573\n",
      "Epoch 86/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1218 - val_loss: 0.5576\n",
      "Epoch 87/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1212 - val_loss: 0.5571\n",
      "Epoch 88/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1209 - val_loss: 0.5568\n",
      "Epoch 89/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1205 - val_loss: 0.5571\n",
      "Epoch 90/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1206 - val_loss: 0.5569\n",
      "Epoch 91/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1213 - val_loss: 0.5570\n",
      "Epoch 92/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1210 - val_loss: 0.5576\n",
      "Epoch 93/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1208 - val_loss: 0.5570\n",
      "Epoch 94/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1212 - val_loss: 0.5571\n",
      "Epoch 95/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1208 - val_loss: 0.5575\n",
      "Epoch 96/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1203 - val_loss: 0.5578\n",
      "Epoch 97/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1215 - val_loss: 0.5581\n",
      "Epoch 98/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1216 - val_loss: 0.5587\n",
      "Epoch 99/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1210 - val_loss: 0.5583\n",
      "Epoch 100/1000\n",
      " 500/1248 [===========>..................] - ETA: 0s - loss: 0.1213\n",
      "Epoch 00100: saving model to ./model/LSTM_03_check_point/cp-0100.ckpt\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.SGD object at 0x0000021ACD4FF4E0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "WARNING:tensorflow:From C:\\Users\\DVLAB\\AppData\\anaconda3\\envs\\dlib\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
      "1248/1248 [==============================] - 0s 213us/sample - loss: 0.1207 - val_loss: 0.5581\n",
      "Epoch 101/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1199 - val_loss: 0.5573\n",
      "Epoch 102/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1204 - val_loss: 0.5586\n",
      "Epoch 103/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1203 - val_loss: 0.5591\n",
      "Epoch 104/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1210 - val_loss: 0.5599\n",
      "Epoch 105/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1208 - val_loss: 0.5604\n",
      "Epoch 106/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1206 - val_loss: 0.5600\n",
      "Epoch 107/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1210 - val_loss: 0.5593\n",
      "Epoch 108/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1201 - val_loss: 0.5590\n",
      "Epoch 109/1000\n",
      "1248/1248 [==============================] - 0s 100us/sample - loss: 0.1203 - val_loss: 0.5575\n",
      "Epoch 110/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1199 - val_loss: 0.5570\n",
      "Epoch 111/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1205 - val_loss: 0.5576\n",
      "Epoch 112/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1204 - val_loss: 0.5579\n",
      "Epoch 113/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1204 - val_loss: 0.5575\n",
      "Epoch 114/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1203 - val_loss: 0.5575\n",
      "Epoch 115/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1205 - val_loss: 0.5572\n",
      "Epoch 116/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1199 - val_loss: 0.5578\n",
      "Epoch 117/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1202 - val_loss: 0.5578\n",
      "Epoch 118/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1198 - val_loss: 0.5580\n",
      "Epoch 119/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1193 - val_loss: 0.5568\n",
      "Epoch 120/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1204 - val_loss: 0.5578\n",
      "Epoch 121/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1200 - val_loss: 0.5573\n",
      "Epoch 122/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1196 - val_loss: 0.5574\n",
      "Epoch 123/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1192 - val_loss: 0.5576\n",
      "Epoch 124/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1203 - val_loss: 0.5570\n",
      "Epoch 125/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1198 - val_loss: 0.5573\n",
      "Epoch 126/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1200 - val_loss: 0.5582\n",
      "Epoch 127/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1198 - val_loss: 0.5580\n",
      "Epoch 128/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1197 - val_loss: 0.5588\n",
      "Epoch 129/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1198 - val_loss: 0.5584\n",
      "Epoch 130/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1196 - val_loss: 0.5576\n",
      "Epoch 131/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1197 - val_loss: 0.5572\n",
      "Epoch 132/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1197 - val_loss: 0.5582\n",
      "Epoch 133/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1195 - val_loss: 0.5588\n",
      "Epoch 134/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1202 - val_loss: 0.5600\n",
      "Epoch 135/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1198 - val_loss: 0.5589\n",
      "Epoch 136/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1195 - val_loss: 0.5579\n",
      "Epoch 137/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1196 - val_loss: 0.5573\n",
      "Epoch 138/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1196 - val_loss: 0.5579\n",
      "Epoch 139/1000\n",
      "1248/1248 [==============================] - 0s 100us/sample - loss: 0.1201 - val_loss: 0.5575\n",
      "Epoch 140/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1189 - val_loss: 0.5569\n",
      "Epoch 141/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1196 - val_loss: 0.5582\n",
      "Epoch 142/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1196 - val_loss: 0.5577\n",
      "Epoch 143/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1197 - val_loss: 0.5580\n",
      "Epoch 144/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1195 - val_loss: 0.5575\n",
      "Epoch 145/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1190 - val_loss: 0.5573\n",
      "Epoch 146/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1190 - val_loss: 0.5566\n",
      "Epoch 147/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1195 - val_loss: 0.5571\n",
      "Epoch 148/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1191 - val_loss: 0.5583\n",
      "Epoch 149/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1196 - val_loss: 0.5584\n",
      "Epoch 150/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1194 - val_loss: 0.5581\n",
      "Epoch 151/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1196 - val_loss: 0.5574\n",
      "Epoch 152/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1192 - val_loss: 0.5569\n",
      "Epoch 153/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1194 - val_loss: 0.5573\n",
      "Epoch 154/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1192 - val_loss: 0.5572\n",
      "Epoch 155/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1194 - val_loss: 0.5581\n",
      "Epoch 156/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1189 - val_loss: 0.5578\n",
      "Epoch 157/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1193 - val_loss: 0.5579\n",
      "Epoch 158/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1191 - val_loss: 0.5579\n",
      "Epoch 159/1000\n",
      "1248/1248 [==============================] - 0s 100us/sample - loss: 0.1196 - val_loss: 0.5581\n",
      "Epoch 160/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1190 - val_loss: 0.5586\n",
      "Epoch 161/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1184 - val_loss: 0.5589\n",
      "Epoch 162/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1188 - val_loss: 0.5580\n",
      "Epoch 163/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1186 - val_loss: 0.5587\n",
      "Epoch 164/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1189 - val_loss: 0.5574\n",
      "Epoch 165/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1183 - val_loss: 0.5575\n",
      "Epoch 166/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1191 - val_loss: 0.5579\n",
      "Epoch 167/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1187 - val_loss: 0.5579\n",
      "Epoch 168/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1182 - val_loss: 0.5572\n",
      "Epoch 169/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1179 - val_loss: 0.5574\n",
      "Epoch 170/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1187 - val_loss: 0.5569\n",
      "Epoch 171/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1197 - val_loss: 0.5563\n",
      "Epoch 172/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1183 - val_loss: 0.5568\n",
      "Epoch 173/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1190 - val_loss: 0.5573\n",
      "Epoch 174/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1192 - val_loss: 0.5565\n",
      "Epoch 175/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1181 - val_loss: 0.5563\n",
      "Epoch 176/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1183 - val_loss: 0.5564\n",
      "Epoch 177/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1181 - val_loss: 0.5559\n",
      "Epoch 178/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1191 - val_loss: 0.5560\n",
      "Epoch 179/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1188 - val_loss: 0.5562\n",
      "Epoch 180/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1182 - val_loss: 0.5569\n",
      "Epoch 181/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1187 - val_loss: 0.5567\n",
      "Epoch 182/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1181 - val_loss: 0.5567\n",
      "Epoch 183/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1182 - val_loss: 0.5559\n",
      "Epoch 184/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1176 - val_loss: 0.5570\n",
      "Epoch 185/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1186 - val_loss: 0.5565\n",
      "Epoch 186/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1179 - val_loss: 0.5576\n",
      "Epoch 187/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1188 - val_loss: 0.5574\n",
      "Epoch 188/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1183 - val_loss: 0.5568\n",
      "Epoch 189/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1182 - val_loss: 0.5567\n",
      "Epoch 190/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1181 - val_loss: 0.5569\n",
      "Epoch 191/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1192 - val_loss: 0.5574\n",
      "Epoch 192/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1182 - val_loss: 0.5583\n",
      "Epoch 193/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1183 - val_loss: 0.5579\n",
      "Epoch 194/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1179 - val_loss: 0.5568\n",
      "Epoch 195/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1191 - val_loss: 0.5568\n",
      "Epoch 196/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1184 - val_loss: 0.5562\n",
      "Epoch 197/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1184 - val_loss: 0.5561\n",
      "Epoch 198/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1183 - val_loss: 0.5565\n",
      "Epoch 199/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1177 - val_loss: 0.5572\n",
      "Epoch 200/1000\n",
      " 500/1248 [===========>..................] - ETA: 0s - loss: 0.1074\n",
      "Epoch 00200: saving model to ./model/LSTM_03_check_point/cp-0200.ckpt\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.SGD object at 0x0000021ACD4FF4E0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "1248/1248 [==============================] - 0s 138us/sample - loss: 0.1165 - val_loss: 0.5574\n",
      "Epoch 201/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1178 - val_loss: 0.5573\n",
      "Epoch 202/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1179 - val_loss: 0.5578\n",
      "Epoch 203/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1174 - val_loss: 0.5584\n",
      "Epoch 204/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1174 - val_loss: 0.5578\n",
      "Epoch 205/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1168 - val_loss: 0.5588\n",
      "Epoch 206/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1186 - val_loss: 0.5587\n",
      "Epoch 207/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1178 - val_loss: 0.5579\n",
      "Epoch 208/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1189 - val_loss: 0.5580\n",
      "Epoch 209/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1180 - val_loss: 0.5580\n",
      "Epoch 210/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1182 - val_loss: 0.5561\n",
      "Epoch 211/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1174 - val_loss: 0.5566\n",
      "Epoch 212/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1167 - val_loss: 0.5552\n",
      "Epoch 213/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1176 - val_loss: 0.5552\n",
      "Epoch 214/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1171 - val_loss: 0.5551\n",
      "Epoch 215/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1181 - val_loss: 0.5551\n",
      "Epoch 216/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1171 - val_loss: 0.5558\n",
      "Epoch 217/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1166 - val_loss: 0.5561\n",
      "Epoch 218/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1168 - val_loss: 0.5565\n",
      "Epoch 219/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1183 - val_loss: 0.5556\n",
      "Epoch 220/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1174 - val_loss: 0.5563\n",
      "Epoch 221/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1174 - val_loss: 0.5558\n",
      "Epoch 222/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1171 - val_loss: 0.5571\n",
      "Epoch 223/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1170 - val_loss: 0.5576\n",
      "Epoch 224/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1171 - val_loss: 0.5563\n",
      "Epoch 225/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1163 - val_loss: 0.5564\n",
      "Epoch 226/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1172 - val_loss: 0.5559\n",
      "Epoch 227/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1171 - val_loss: 0.5554\n",
      "Epoch 228/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1160 - val_loss: 0.5564\n",
      "Epoch 229/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1181 - val_loss: 0.5560\n",
      "Epoch 230/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1167 - val_loss: 0.5561\n",
      "Epoch 231/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1166 - val_loss: 0.5570\n",
      "Epoch 232/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1167 - val_loss: 0.5561\n",
      "Epoch 233/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1180 - val_loss: 0.5562\n",
      "Epoch 234/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1175 - val_loss: 0.5570\n",
      "Epoch 235/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1171 - val_loss: 0.5572\n",
      "Epoch 236/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1168 - val_loss: 0.5565\n",
      "Epoch 237/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1175 - val_loss: 0.5564\n",
      "Epoch 238/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1168 - val_loss: 0.5571\n",
      "Epoch 239/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1175 - val_loss: 0.5577\n",
      "Epoch 240/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1165 - val_loss: 0.5569\n",
      "Epoch 241/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1164 - val_loss: 0.5562\n",
      "Epoch 242/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1173 - val_loss: 0.5556\n",
      "Epoch 243/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1161 - val_loss: 0.5556\n",
      "Epoch 244/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1166 - val_loss: 0.5555\n",
      "Epoch 245/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1172 - val_loss: 0.5550\n",
      "Epoch 246/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1166 - val_loss: 0.5555\n",
      "Epoch 247/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1164 - val_loss: 0.5555\n",
      "Epoch 248/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1164 - val_loss: 0.5565\n",
      "Epoch 249/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1170 - val_loss: 0.5564\n",
      "Epoch 250/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1172 - val_loss: 0.5557\n",
      "Epoch 251/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1158 - val_loss: 0.5562\n",
      "Epoch 252/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1169 - val_loss: 0.5549\n",
      "Epoch 253/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1164 - val_loss: 0.5546\n",
      "Epoch 254/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1168 - val_loss: 0.5547\n",
      "Epoch 255/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1167 - val_loss: 0.5546\n",
      "Epoch 256/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1174 - val_loss: 0.5552\n",
      "Epoch 257/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1156 - val_loss: 0.5555\n",
      "Epoch 258/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1161 - val_loss: 0.5548\n",
      "Epoch 259/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1174 - val_loss: 0.5552\n",
      "Epoch 260/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1162 - val_loss: 0.5554\n",
      "Epoch 261/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1168 - val_loss: 0.5551\n",
      "Epoch 262/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1164 - val_loss: 0.5548\n",
      "Epoch 263/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1169 - val_loss: 0.5539\n",
      "Epoch 264/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1166 - val_loss: 0.5545\n",
      "Epoch 265/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1166 - val_loss: 0.5539\n",
      "Epoch 266/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1158 - val_loss: 0.5553\n",
      "Epoch 267/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1165 - val_loss: 0.5569\n",
      "Epoch 268/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1166 - val_loss: 0.5567\n",
      "Epoch 269/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1174 - val_loss: 0.5559\n",
      "Epoch 270/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1158 - val_loss: 0.5558\n",
      "Epoch 271/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1152 - val_loss: 0.5550\n",
      "Epoch 272/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1160 - val_loss: 0.5547\n",
      "Epoch 273/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1163 - val_loss: 0.5544\n",
      "Epoch 274/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1160 - val_loss: 0.5541\n",
      "Epoch 275/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1161 - val_loss: 0.5542\n",
      "Epoch 276/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1166 - val_loss: 0.5542\n",
      "Epoch 277/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1157 - val_loss: 0.5544\n",
      "Epoch 278/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1162 - val_loss: 0.5548\n",
      "Epoch 279/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1156 - val_loss: 0.5551\n",
      "Epoch 280/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1160 - val_loss: 0.5548\n",
      "Epoch 281/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1159 - val_loss: 0.5557\n",
      "Epoch 282/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1152 - val_loss: 0.5547\n",
      "Epoch 283/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1169 - val_loss: 0.5545\n",
      "Epoch 284/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1158 - val_loss: 0.5549\n",
      "Epoch 285/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1156 - val_loss: 0.5556\n",
      "Epoch 286/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1159 - val_loss: 0.5563\n",
      "Epoch 287/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1152 - val_loss: 0.5550\n",
      "Epoch 288/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1168 - val_loss: 0.5549\n",
      "Epoch 289/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1158 - val_loss: 0.5547\n",
      "Epoch 290/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1165 - val_loss: 0.5542\n",
      "Epoch 291/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1138 - val_loss: 0.5538\n",
      "Epoch 292/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1166 - val_loss: 0.5544\n",
      "Epoch 293/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1163 - val_loss: 0.5530\n",
      "Epoch 294/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1150 - val_loss: 0.5532\n",
      "Epoch 295/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1169 - val_loss: 0.5532\n",
      "Epoch 296/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1154 - val_loss: 0.5548\n",
      "Epoch 297/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1156 - val_loss: 0.5539\n",
      "Epoch 298/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1151 - val_loss: 0.5543\n",
      "Epoch 299/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1154 - val_loss: 0.5537\n",
      "Epoch 300/1000\n",
      " 500/1248 [===========>..................] - ETA: 0s - loss: 0.1207\n",
      "Epoch 00300: saving model to ./model/LSTM_03_check_point/cp-0300.ckpt\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.SGD object at 0x0000021ACD4FF4E0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "1248/1248 [==============================] - 0s 138us/sample - loss: 0.1147 - val_loss: 0.5545\n",
      "Epoch 301/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1159 - val_loss: 0.5552\n",
      "Epoch 302/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1154 - val_loss: 0.5550\n",
      "Epoch 303/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1158 - val_loss: 0.5545\n",
      "Epoch 304/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1155 - val_loss: 0.5538\n",
      "Epoch 305/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1149 - val_loss: 0.5540\n",
      "Epoch 306/1000\n",
      "1248/1248 [==============================] - 0s 100us/sample - loss: 0.1152 - val_loss: 0.5543\n",
      "Epoch 307/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1145 - val_loss: 0.5545\n",
      "Epoch 308/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1143 - val_loss: 0.5543\n",
      "Epoch 309/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1166 - val_loss: 0.5525\n",
      "Epoch 310/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1158 - val_loss: 0.5525\n",
      "Epoch 311/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1153 - val_loss: 0.5523\n",
      "Epoch 312/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1148 - val_loss: 0.5513\n",
      "Epoch 313/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1153 - val_loss: 0.5527\n",
      "Epoch 314/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1151 - val_loss: 0.5525\n",
      "Epoch 315/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1144 - val_loss: 0.5527\n",
      "Epoch 316/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1148 - val_loss: 0.5518\n",
      "Epoch 317/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1149 - val_loss: 0.5533\n",
      "Epoch 318/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1156 - val_loss: 0.5533\n",
      "Epoch 319/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1163 - val_loss: 0.5531\n",
      "Epoch 320/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1135 - val_loss: 0.5549\n",
      "Epoch 321/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1152 - val_loss: 0.5551\n",
      "Epoch 322/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1146 - val_loss: 0.5548\n",
      "Epoch 323/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1128 - val_loss: 0.5542\n",
      "Epoch 324/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1148 - val_loss: 0.5537\n",
      "Epoch 325/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1144 - val_loss: 0.5538\n",
      "Epoch 326/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1158 - val_loss: 0.5530\n",
      "Epoch 327/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1149 - val_loss: 0.5516\n",
      "Epoch 328/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1143 - val_loss: 0.5512\n",
      "Epoch 329/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1153 - val_loss: 0.5517\n",
      "Epoch 330/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1142 - val_loss: 0.5527\n",
      "Epoch 331/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1146 - val_loss: 0.5531\n",
      "Epoch 332/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1159 - val_loss: 0.5514\n",
      "Epoch 333/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1148 - val_loss: 0.5517\n",
      "Epoch 334/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1146 - val_loss: 0.5528\n",
      "Epoch 335/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1147 - val_loss: 0.5521\n",
      "Epoch 336/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1150 - val_loss: 0.5529\n",
      "Epoch 337/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1142 - val_loss: 0.5529\n",
      "Epoch 338/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1151 - val_loss: 0.5534\n",
      "Epoch 339/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1143 - val_loss: 0.5539\n",
      "Epoch 340/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1139 - val_loss: 0.5532\n",
      "Epoch 341/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1146 - val_loss: 0.5524\n",
      "Epoch 342/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1145 - val_loss: 0.5525\n",
      "Epoch 343/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1153 - val_loss: 0.5515\n",
      "Epoch 344/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1149 - val_loss: 0.5529\n",
      "Epoch 345/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1134 - val_loss: 0.5523\n",
      "Epoch 346/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1137 - val_loss: 0.5517\n",
      "Epoch 347/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1145 - val_loss: 0.5526\n",
      "Epoch 348/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1137 - val_loss: 0.5533\n",
      "Epoch 349/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1140 - val_loss: 0.5543\n",
      "Epoch 350/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1147 - val_loss: 0.5545\n",
      "Epoch 351/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1142 - val_loss: 0.5541\n",
      "Epoch 352/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1141 - val_loss: 0.5533\n",
      "Epoch 353/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1146 - val_loss: 0.5532\n",
      "Epoch 354/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1132 - val_loss: 0.5523\n",
      "Epoch 355/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1141 - val_loss: 0.5525\n",
      "Epoch 356/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1148 - val_loss: 0.5518\n",
      "Epoch 357/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1141 - val_loss: 0.5519\n",
      "Epoch 358/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1141 - val_loss: 0.5538\n",
      "Epoch 359/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1144 - val_loss: 0.5533\n",
      "Epoch 360/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1135 - val_loss: 0.5528\n",
      "Epoch 361/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1126 - val_loss: 0.5538\n",
      "Epoch 362/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1133 - val_loss: 0.5537\n",
      "Epoch 363/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1147 - val_loss: 0.5532\n",
      "Epoch 364/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1136 - val_loss: 0.5517\n",
      "Epoch 365/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1149 - val_loss: 0.5527\n",
      "Epoch 366/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1145 - val_loss: 0.5520\n",
      "Epoch 367/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1136 - val_loss: 0.5519\n",
      "Epoch 368/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1145 - val_loss: 0.5512\n",
      "Epoch 369/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1136 - val_loss: 0.5520\n",
      "Epoch 370/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1129 - val_loss: 0.5525\n",
      "Epoch 371/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1133 - val_loss: 0.5527\n",
      "Epoch 372/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1128 - val_loss: 0.5525\n",
      "Epoch 373/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1144 - val_loss: 0.5523\n",
      "Epoch 374/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1131 - val_loss: 0.5513\n",
      "Epoch 375/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1143 - val_loss: 0.5525\n",
      "Epoch 376/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1138 - val_loss: 0.5511\n",
      "Epoch 377/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1134 - val_loss: 0.5527\n",
      "Epoch 378/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1137 - val_loss: 0.5525\n",
      "Epoch 379/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1137 - val_loss: 0.5515\n",
      "Epoch 380/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1130 - val_loss: 0.5515\n",
      "Epoch 381/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1148 - val_loss: 0.5508\n",
      "Epoch 382/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1137 - val_loss: 0.5520\n",
      "Epoch 383/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1131 - val_loss: 0.5521\n",
      "Epoch 384/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1128 - val_loss: 0.5532\n",
      "Epoch 385/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1136 - val_loss: 0.5533\n",
      "Epoch 386/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1133 - val_loss: 0.5515\n",
      "Epoch 387/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1129 - val_loss: 0.5513\n",
      "Epoch 388/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1124 - val_loss: 0.5509\n",
      "Epoch 389/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1126 - val_loss: 0.5515\n",
      "Epoch 390/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1131 - val_loss: 0.5512\n",
      "Epoch 391/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1133 - val_loss: 0.5509\n",
      "Epoch 392/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1119 - val_loss: 0.5511\n",
      "Epoch 393/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1134 - val_loss: 0.5510\n",
      "Epoch 394/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1130 - val_loss: 0.5517\n",
      "Epoch 395/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1139 - val_loss: 0.5517\n",
      "Epoch 396/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1127 - val_loss: 0.5517\n",
      "Epoch 397/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1132 - val_loss: 0.5515\n",
      "Epoch 398/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1114 - val_loss: 0.5520\n",
      "Epoch 399/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1127 - val_loss: 0.5509\n",
      "Epoch 400/1000\n",
      " 500/1248 [===========>..................] - ETA: 0s - loss: 0.1083\n",
      "Epoch 00400: saving model to ./model/LSTM_03_check_point/cp-0400.ckpt\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.SGD object at 0x0000021ACD4FF4E0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "1248/1248 [==============================] - 0s 163us/sample - loss: 0.1129 - val_loss: 0.5513\n",
      "Epoch 401/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1133 - val_loss: 0.5508\n",
      "Epoch 402/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1127 - val_loss: 0.5501\n",
      "Epoch 403/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1118 - val_loss: 0.5520\n",
      "Epoch 404/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1138 - val_loss: 0.5515\n",
      "Epoch 405/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1134 - val_loss: 0.5503\n",
      "Epoch 406/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1122 - val_loss: 0.5497\n",
      "Epoch 407/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1142 - val_loss: 0.5502\n",
      "Epoch 408/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1126 - val_loss: 0.5503\n",
      "Epoch 409/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1121 - val_loss: 0.5507\n",
      "Epoch 410/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1132 - val_loss: 0.5507\n",
      "Epoch 411/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1133 - val_loss: 0.5513\n",
      "Epoch 412/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1141 - val_loss: 0.5507\n",
      "Epoch 413/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1115 - val_loss: 0.5516\n",
      "Epoch 414/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1131 - val_loss: 0.5520\n",
      "Epoch 415/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1125 - val_loss: 0.5518\n",
      "Epoch 416/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1132 - val_loss: 0.5506\n",
      "Epoch 417/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1136 - val_loss: 0.5503\n",
      "Epoch 418/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1140 - val_loss: 0.5505\n",
      "Epoch 419/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1118 - val_loss: 0.5493\n",
      "Epoch 420/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1123 - val_loss: 0.5488\n",
      "Epoch 421/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1117 - val_loss: 0.5503\n",
      "Epoch 422/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1117 - val_loss: 0.5500\n",
      "Epoch 423/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1123 - val_loss: 0.5501\n",
      "Epoch 424/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1138 - val_loss: 0.5495\n",
      "Epoch 425/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1122 - val_loss: 0.5497\n",
      "Epoch 426/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1140 - val_loss: 0.5487\n",
      "Epoch 427/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1126 - val_loss: 0.5494\n",
      "Epoch 428/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1117 - val_loss: 0.5500\n",
      "Epoch 429/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1123 - val_loss: 0.5510\n",
      "Epoch 430/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1128 - val_loss: 0.5502\n",
      "Epoch 431/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1126 - val_loss: 0.5496\n",
      "Epoch 432/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1117 - val_loss: 0.5498\n",
      "Epoch 433/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1127 - val_loss: 0.5493\n",
      "Epoch 434/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1129 - val_loss: 0.5504\n",
      "Epoch 435/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1119 - val_loss: 0.5520\n",
      "Epoch 436/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1140 - val_loss: 0.5520\n",
      "Epoch 437/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1116 - val_loss: 0.5501\n",
      "Epoch 438/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1122 - val_loss: 0.5507\n",
      "Epoch 439/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1120 - val_loss: 0.5501\n",
      "Epoch 440/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1121 - val_loss: 0.5498\n",
      "Epoch 441/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1112 - val_loss: 0.5502\n",
      "Epoch 442/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1123 - val_loss: 0.5509\n",
      "Epoch 443/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1124 - val_loss: 0.5506\n",
      "Epoch 444/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1117 - val_loss: 0.5501\n",
      "Epoch 445/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1120 - val_loss: 0.5495\n",
      "Epoch 446/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1130 - val_loss: 0.5488\n",
      "Epoch 447/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1117 - val_loss: 0.5491\n",
      "Epoch 448/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1118 - val_loss: 0.5487\n",
      "Epoch 449/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1122 - val_loss: 0.5494\n",
      "Epoch 450/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1120 - val_loss: 0.5496\n",
      "Epoch 451/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1134 - val_loss: 0.5489\n",
      "Epoch 452/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1117 - val_loss: 0.5490\n",
      "Epoch 453/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1108 - val_loss: 0.5486\n",
      "Epoch 454/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1117 - val_loss: 0.5484\n",
      "Epoch 455/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1113 - val_loss: 0.5502\n",
      "Epoch 456/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1125 - val_loss: 0.5486\n",
      "Epoch 457/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1136 - val_loss: 0.5477\n",
      "Epoch 458/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1130 - val_loss: 0.5472\n",
      "Epoch 459/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1106 - val_loss: 0.5489\n",
      "Epoch 460/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1122 - val_loss: 0.5490\n",
      "Epoch 461/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1116 - val_loss: 0.5492\n",
      "Epoch 462/1000\n",
      "1248/1248 [==============================] - 0s 100us/sample - loss: 0.1113 - val_loss: 0.5498\n",
      "Epoch 463/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1104 - val_loss: 0.5497\n",
      "Epoch 464/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1108 - val_loss: 0.5506\n",
      "Epoch 465/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1113 - val_loss: 0.5507\n",
      "Epoch 466/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1108 - val_loss: 0.5508\n",
      "Epoch 467/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1109 - val_loss: 0.5493\n",
      "Epoch 468/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1107 - val_loss: 0.5510\n",
      "Epoch 469/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1119 - val_loss: 0.5506\n",
      "Epoch 470/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1122 - val_loss: 0.5508\n",
      "Epoch 471/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1115 - val_loss: 0.5489\n",
      "Epoch 472/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1119 - val_loss: 0.5487\n",
      "Epoch 473/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1105 - val_loss: 0.5492\n",
      "Epoch 474/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1112 - val_loss: 0.5478\n",
      "Epoch 475/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1109 - val_loss: 0.5476\n",
      "Epoch 476/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1105 - val_loss: 0.5483\n",
      "Epoch 477/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1107 - val_loss: 0.5502\n",
      "Epoch 478/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1103 - val_loss: 0.5494\n",
      "Epoch 479/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1100 - val_loss: 0.5493\n",
      "Epoch 480/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1121 - val_loss: 0.5488\n",
      "Epoch 481/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1116 - val_loss: 0.5491\n",
      "Epoch 482/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1115 - val_loss: 0.5481\n",
      "Epoch 483/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1116 - val_loss: 0.5481\n",
      "Epoch 484/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1094 - val_loss: 0.5495\n",
      "Epoch 485/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1119 - val_loss: 0.5503\n",
      "Epoch 486/1000\n",
      "1248/1248 [==============================] - 0s 100us/sample - loss: 0.1107 - val_loss: 0.5474\n",
      "Epoch 487/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1121 - val_loss: 0.5474\n",
      "Epoch 488/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1096 - val_loss: 0.5494\n",
      "Epoch 489/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1112 - val_loss: 0.5486\n",
      "Epoch 490/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1105 - val_loss: 0.5497\n",
      "Epoch 491/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1115 - val_loss: 0.5490\n",
      "Epoch 492/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1114 - val_loss: 0.5475\n",
      "Epoch 493/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1105 - val_loss: 0.5485\n",
      "Epoch 494/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1113 - val_loss: 0.5480\n",
      "Epoch 495/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1109 - val_loss: 0.5486\n",
      "Epoch 496/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1105 - val_loss: 0.5492\n",
      "Epoch 497/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1098 - val_loss: 0.5484\n",
      "Epoch 498/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1107 - val_loss: 0.5489\n",
      "Epoch 499/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1111 - val_loss: 0.5494\n",
      "Epoch 500/1000\n",
      " 500/1248 [===========>..................] - ETA: 0s - loss: 0.0968\n",
      "Epoch 00500: saving model to ./model/LSTM_03_check_point/cp-0500.ckpt\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.SGD object at 0x0000021ACD4FF4E0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "1248/1248 [==============================] - 0s 150us/sample - loss: 0.1101 - val_loss: 0.5493\n",
      "Epoch 501/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1113 - val_loss: 0.5472\n",
      "Epoch 502/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1105 - val_loss: 0.5467\n",
      "Epoch 503/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1112 - val_loss: 0.5477\n",
      "Epoch 504/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1100 - val_loss: 0.5477\n",
      "Epoch 505/1000\n",
      "1248/1248 [==============================] - 0s 100us/sample - loss: 0.1101 - val_loss: 0.5472\n",
      "Epoch 506/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1119 - val_loss: 0.5469\n",
      "Epoch 507/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1105 - val_loss: 0.5458\n",
      "Epoch 508/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1114 - val_loss: 0.5462\n",
      "Epoch 509/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1094 - val_loss: 0.5468\n",
      "Epoch 510/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1105 - val_loss: 0.5481\n",
      "Epoch 511/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1109 - val_loss: 0.5478\n",
      "Epoch 512/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1097 - val_loss: 0.5475\n",
      "Epoch 513/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1098 - val_loss: 0.5485\n",
      "Epoch 514/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1099 - val_loss: 0.5481\n",
      "Epoch 515/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1107 - val_loss: 0.5479\n",
      "Epoch 516/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1109 - val_loss: 0.5473\n",
      "Epoch 517/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1104 - val_loss: 0.5477\n",
      "Epoch 518/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1090 - val_loss: 0.5489\n",
      "Epoch 519/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1108 - val_loss: 0.5480\n",
      "Epoch 520/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1103 - val_loss: 0.5489\n",
      "Epoch 521/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1101 - val_loss: 0.5485\n",
      "Epoch 522/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1094 - val_loss: 0.5460\n",
      "Epoch 523/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1087 - val_loss: 0.5456\n",
      "Epoch 524/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1096 - val_loss: 0.5467\n",
      "Epoch 525/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1095 - val_loss: 0.5472\n",
      "Epoch 526/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1091 - val_loss: 0.5470\n",
      "Epoch 527/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1091 - val_loss: 0.5458\n",
      "Epoch 528/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1096 - val_loss: 0.5459\n",
      "Epoch 529/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1104 - val_loss: 0.5461\n",
      "Epoch 530/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1096 - val_loss: 0.5467\n",
      "Epoch 531/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1095 - val_loss: 0.5456\n",
      "Epoch 532/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1111 - val_loss: 0.5458\n",
      "Epoch 533/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1099 - val_loss: 0.5463\n",
      "Epoch 534/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1093 - val_loss: 0.5465\n",
      "Epoch 535/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1097 - val_loss: 0.5460\n",
      "Epoch 536/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1097 - val_loss: 0.5463\n",
      "Epoch 537/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1107 - val_loss: 0.5454\n",
      "Epoch 538/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1103 - val_loss: 0.5470\n",
      "Epoch 539/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1089 - val_loss: 0.5469\n",
      "Epoch 540/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1111 - val_loss: 0.5469\n",
      "Epoch 541/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1097 - val_loss: 0.5472\n",
      "Epoch 542/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1102 - val_loss: 0.5476\n",
      "Epoch 543/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1101 - val_loss: 0.5465\n",
      "Epoch 544/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1095 - val_loss: 0.5458\n",
      "Epoch 545/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1089 - val_loss: 0.5455\n",
      "Epoch 546/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1102 - val_loss: 0.5461\n",
      "Epoch 547/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1090 - val_loss: 0.5452\n",
      "Epoch 548/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1081 - val_loss: 0.5461\n",
      "Epoch 549/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1093 - val_loss: 0.5453\n",
      "Epoch 550/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1085 - val_loss: 0.5458\n",
      "Epoch 551/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1084 - val_loss: 0.5458\n",
      "Epoch 552/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1084 - val_loss: 0.5462\n",
      "Epoch 553/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1098 - val_loss: 0.5462\n",
      "Epoch 554/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1089 - val_loss: 0.5461\n",
      "Epoch 555/1000\n",
      "1248/1248 [==============================] - 0s 100us/sample - loss: 0.1087 - val_loss: 0.5461\n",
      "Epoch 556/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1107 - val_loss: 0.5453\n",
      "Epoch 557/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1091 - val_loss: 0.5449\n",
      "Epoch 558/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1075 - val_loss: 0.5460\n",
      "Epoch 559/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1077 - val_loss: 0.5460\n",
      "Epoch 560/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1092 - val_loss: 0.5472\n",
      "Epoch 561/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1078 - val_loss: 0.5460\n",
      "Epoch 562/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1085 - val_loss: 0.5461\n",
      "Epoch 563/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1100 - val_loss: 0.5460\n",
      "Epoch 564/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1085 - val_loss: 0.5450\n",
      "Epoch 565/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1090 - val_loss: 0.5442\n",
      "Epoch 566/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1081 - val_loss: 0.5456\n",
      "Epoch 567/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1099 - val_loss: 0.5442\n",
      "Epoch 568/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1073 - val_loss: 0.5454\n",
      "Epoch 569/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1079 - val_loss: 0.5446\n",
      "Epoch 570/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1073 - val_loss: 0.5442\n",
      "Epoch 571/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1075 - val_loss: 0.5452\n",
      "Epoch 572/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1087 - val_loss: 0.5452\n",
      "Epoch 573/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1094 - val_loss: 0.5447\n",
      "Epoch 574/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1071 - val_loss: 0.5461\n",
      "Epoch 575/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1081 - val_loss: 0.5457\n",
      "Epoch 576/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1098 - val_loss: 0.5440\n",
      "Epoch 577/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1079 - val_loss: 0.5446\n",
      "Epoch 578/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1079 - val_loss: 0.5449\n",
      "Epoch 579/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1069 - val_loss: 0.5455\n",
      "Epoch 580/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1083 - val_loss: 0.5447\n",
      "Epoch 581/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1090 - val_loss: 0.5441\n",
      "Epoch 582/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1093 - val_loss: 0.5448\n",
      "Epoch 583/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1067 - val_loss: 0.5446\n",
      "Epoch 584/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1069 - val_loss: 0.5452\n",
      "Epoch 585/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1077 - val_loss: 0.5451\n",
      "Epoch 586/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1083 - val_loss: 0.5451\n",
      "Epoch 587/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1072 - val_loss: 0.5445\n",
      "Epoch 588/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1070 - val_loss: 0.5455\n",
      "Epoch 589/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1095 - val_loss: 0.5446\n",
      "Epoch 590/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1074 - val_loss: 0.5432\n",
      "Epoch 591/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1063 - val_loss: 0.5421\n",
      "Epoch 592/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1065 - val_loss: 0.5439\n",
      "Epoch 593/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1072 - val_loss: 0.5437\n",
      "Epoch 594/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1088 - val_loss: 0.5431\n",
      "Epoch 595/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1073 - val_loss: 0.5437\n",
      "Epoch 596/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1072 - val_loss: 0.5438\n",
      "Epoch 597/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1073 - val_loss: 0.5439\n",
      "Epoch 598/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1065 - val_loss: 0.5447\n",
      "Epoch 599/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1078 - val_loss: 0.5444\n",
      "Epoch 600/1000\n",
      " 500/1248 [===========>..................] - ETA: 0s - loss: 0.1028\n",
      "Epoch 00600: saving model to ./model/LSTM_03_check_point/cp-0600.ckpt\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.SGD object at 0x0000021ACD4FF4E0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "1248/1248 [==============================] - 0s 150us/sample - loss: 0.1082 - val_loss: 0.5466\n",
      "Epoch 601/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1092 - val_loss: 0.5447\n",
      "Epoch 602/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1072 - val_loss: 0.5443\n",
      "Epoch 603/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1078 - val_loss: 0.5440\n",
      "Epoch 604/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1075 - val_loss: 0.5444\n",
      "Epoch 605/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1061 - val_loss: 0.5450\n",
      "Epoch 606/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1072 - val_loss: 0.5452\n",
      "Epoch 607/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1070 - val_loss: 0.5456\n",
      "Epoch 608/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1069 - val_loss: 0.5442\n",
      "Epoch 609/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1076 - val_loss: 0.5439\n",
      "Epoch 610/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1079 - val_loss: 0.5428\n",
      "Epoch 611/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1072 - val_loss: 0.5437\n",
      "Epoch 612/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1084 - val_loss: 0.5436\n",
      "Epoch 613/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1063 - val_loss: 0.5442\n",
      "Epoch 614/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1061 - val_loss: 0.5440\n",
      "Epoch 615/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1076 - val_loss: 0.5425\n",
      "Epoch 616/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1075 - val_loss: 0.5438\n",
      "Epoch 617/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1075 - val_loss: 0.5456\n",
      "Epoch 618/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1064 - val_loss: 0.5441\n",
      "Epoch 619/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1080 - val_loss: 0.5446\n",
      "Epoch 620/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1079 - val_loss: 0.5438\n",
      "Epoch 621/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1077 - val_loss: 0.5431\n",
      "Epoch 622/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1052 - val_loss: 0.5437\n",
      "Epoch 623/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1066 - val_loss: 0.5431\n",
      "Epoch 624/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1076 - val_loss: 0.5428\n",
      "Epoch 625/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1058 - val_loss: 0.5442\n",
      "Epoch 626/1000\n",
      "1248/1248 [==============================] - 0s 100us/sample - loss: 0.1062 - val_loss: 0.5448\n",
      "Epoch 627/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1079 - val_loss: 0.5439\n",
      "Epoch 628/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1063 - val_loss: 0.5440\n",
      "Epoch 629/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1070 - val_loss: 0.5434\n",
      "Epoch 630/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1070 - val_loss: 0.5443\n",
      "Epoch 631/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1080 - val_loss: 0.5442\n",
      "Epoch 632/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1056 - val_loss: 0.5431\n",
      "Epoch 633/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1075 - val_loss: 0.5418\n",
      "Epoch 634/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1059 - val_loss: 0.5424\n",
      "Epoch 635/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1062 - val_loss: 0.5429\n",
      "Epoch 636/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1087 - val_loss: 0.5434\n",
      "Epoch 637/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1080 - val_loss: 0.5428\n",
      "Epoch 638/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1069 - val_loss: 0.5435\n",
      "Epoch 639/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1067 - val_loss: 0.5438\n",
      "Epoch 640/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1054 - val_loss: 0.5438\n",
      "Epoch 641/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1071 - val_loss: 0.5446\n",
      "Epoch 642/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1060 - val_loss: 0.5444\n",
      "Epoch 643/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1071 - val_loss: 0.5436\n",
      "Epoch 644/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1062 - val_loss: 0.5416\n",
      "Epoch 645/1000\n",
      "1248/1248 [==============================] - 0s 100us/sample - loss: 0.1060 - val_loss: 0.5431\n",
      "Epoch 646/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1068 - val_loss: 0.5430\n",
      "Epoch 647/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1051 - val_loss: 0.5435\n",
      "Epoch 648/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1058 - val_loss: 0.5437\n",
      "Epoch 649/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1056 - val_loss: 0.5432\n",
      "Epoch 650/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1075 - val_loss: 0.5439\n",
      "Epoch 651/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1061 - val_loss: 0.5415\n",
      "Epoch 652/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1059 - val_loss: 0.5415\n",
      "Epoch 653/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1062 - val_loss: 0.5420\n",
      "Epoch 654/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1051 - val_loss: 0.5429\n",
      "Epoch 655/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1074 - val_loss: 0.5427\n",
      "Epoch 656/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1053 - val_loss: 0.5420\n",
      "Epoch 657/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1065 - val_loss: 0.5426\n",
      "Epoch 658/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1057 - val_loss: 0.5433\n",
      "Epoch 659/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1063 - val_loss: 0.5425\n",
      "Epoch 660/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1064 - val_loss: 0.5441\n",
      "Epoch 661/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1062 - val_loss: 0.5433\n",
      "Epoch 662/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1059 - val_loss: 0.5428\n",
      "Epoch 663/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1063 - val_loss: 0.5427\n",
      "Epoch 664/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1052 - val_loss: 0.5424\n",
      "Epoch 665/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1062 - val_loss: 0.5421\n",
      "Epoch 666/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1064 - val_loss: 0.5411\n",
      "Epoch 667/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1059 - val_loss: 0.5415\n",
      "Epoch 668/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1053 - val_loss: 0.5416\n",
      "Epoch 669/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1065 - val_loss: 0.5413\n",
      "Epoch 670/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1076 - val_loss: 0.5408\n",
      "Epoch 671/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1049 - val_loss: 0.5413\n",
      "Epoch 672/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1049 - val_loss: 0.5406\n",
      "Epoch 673/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1066 - val_loss: 0.5415\n",
      "Epoch 674/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1060 - val_loss: 0.5412\n",
      "Epoch 675/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1061 - val_loss: 0.5412\n",
      "Epoch 676/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1075 - val_loss: 0.5411\n",
      "Epoch 677/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1067 - val_loss: 0.5409\n",
      "Epoch 678/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1056 - val_loss: 0.5407\n",
      "Epoch 679/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1043 - val_loss: 0.5414\n",
      "Epoch 680/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1071 - val_loss: 0.5423\n",
      "Epoch 681/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1051 - val_loss: 0.5421\n",
      "Epoch 682/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1064 - val_loss: 0.5410\n",
      "Epoch 683/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1059 - val_loss: 0.5411\n",
      "Epoch 684/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1064 - val_loss: 0.5413\n",
      "Epoch 685/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1062 - val_loss: 0.5409\n",
      "Epoch 686/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1061 - val_loss: 0.5415\n",
      "Epoch 687/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1063 - val_loss: 0.5422\n",
      "Epoch 688/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1061 - val_loss: 0.5421\n",
      "Epoch 689/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1047 - val_loss: 0.5414\n",
      "Epoch 690/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1044 - val_loss: 0.5411\n",
      "Epoch 691/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1038 - val_loss: 0.5408\n",
      "Epoch 692/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1050 - val_loss: 0.5399\n",
      "Epoch 693/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1066 - val_loss: 0.5400\n",
      "Epoch 694/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1050 - val_loss: 0.5411\n",
      "Epoch 695/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1051 - val_loss: 0.5426\n",
      "Epoch 696/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1064 - val_loss: 0.5419\n",
      "Epoch 697/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1032 - val_loss: 0.5414\n",
      "Epoch 698/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1046 - val_loss: 0.5408\n",
      "Epoch 699/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1049 - val_loss: 0.5403\n",
      "Epoch 700/1000\n",
      " 500/1248 [===========>..................] - ETA: 0s - loss: 0.1194\n",
      "Epoch 00700: saving model to ./model/LSTM_03_check_point/cp-0700.ckpt\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.SGD object at 0x0000021ACD4FF4E0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "1248/1248 [==============================] - 0s 138us/sample - loss: 0.1032 - val_loss: 0.5422\n",
      "Epoch 701/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1046 - val_loss: 0.5413\n",
      "Epoch 702/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1074 - val_loss: 0.5408\n",
      "Epoch 703/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1050 - val_loss: 0.5416\n",
      "Epoch 704/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1068 - val_loss: 0.5419\n",
      "Epoch 705/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1057 - val_loss: 0.5407\n",
      "Epoch 706/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1034 - val_loss: 0.5409\n",
      "Epoch 707/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1056 - val_loss: 0.5398\n",
      "Epoch 708/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1069 - val_loss: 0.5400\n",
      "Epoch 709/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1051 - val_loss: 0.5403\n",
      "Epoch 710/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1046 - val_loss: 0.5396\n",
      "Epoch 711/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1044 - val_loss: 0.5405\n",
      "Epoch 712/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1058 - val_loss: 0.5411\n",
      "Epoch 713/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1039 - val_loss: 0.5413\n",
      "Epoch 714/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1051 - val_loss: 0.5409\n",
      "Epoch 715/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1058 - val_loss: 0.5416\n",
      "Epoch 716/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1063 - val_loss: 0.5415\n",
      "Epoch 717/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1044 - val_loss: 0.5421\n",
      "Epoch 718/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1055 - val_loss: 0.5398\n",
      "Epoch 719/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1064 - val_loss: 0.5411\n",
      "Epoch 720/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1056 - val_loss: 0.5405\n",
      "Epoch 721/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1045 - val_loss: 0.5398\n",
      "Epoch 722/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1031 - val_loss: 0.5408\n",
      "Epoch 723/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1056 - val_loss: 0.5416\n",
      "Epoch 724/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1040 - val_loss: 0.5419\n",
      "Epoch 725/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1045 - val_loss: 0.5408\n",
      "Epoch 726/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1037 - val_loss: 0.5408\n",
      "Epoch 727/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1050 - val_loss: 0.5402\n",
      "Epoch 728/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1059 - val_loss: 0.5401\n",
      "Epoch 729/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1044 - val_loss: 0.5409\n",
      "Epoch 730/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1049 - val_loss: 0.5408\n",
      "Epoch 731/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1038 - val_loss: 0.5415\n",
      "Epoch 732/1000\n",
      "1248/1248 [==============================] - 0s 100us/sample - loss: 0.1051 - val_loss: 0.5413\n",
      "Epoch 733/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1043 - val_loss: 0.5409\n",
      "Epoch 734/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1039 - val_loss: 0.5412\n",
      "Epoch 735/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1061 - val_loss: 0.5412\n",
      "Epoch 736/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1067 - val_loss: 0.5410\n",
      "Epoch 737/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1045 - val_loss: 0.5418\n",
      "Epoch 738/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1055 - val_loss: 0.5419\n",
      "Epoch 739/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1050 - val_loss: 0.5415\n",
      "Epoch 740/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1050 - val_loss: 0.5414\n",
      "Epoch 741/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1060 - val_loss: 0.5407\n",
      "Epoch 742/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1052 - val_loss: 0.5419\n",
      "Epoch 743/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1038 - val_loss: 0.5417\n",
      "Epoch 744/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1036 - val_loss: 0.5418\n",
      "Epoch 745/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1058 - val_loss: 0.5413\n",
      "Epoch 746/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1034 - val_loss: 0.5420\n",
      "Epoch 747/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1034 - val_loss: 0.5419\n",
      "Epoch 748/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1046 - val_loss: 0.5414\n",
      "Epoch 749/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1024 - val_loss: 0.5417\n",
      "Epoch 750/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1041 - val_loss: 0.5413\n",
      "Epoch 751/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1059 - val_loss: 0.5407\n",
      "Epoch 752/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1055 - val_loss: 0.5404\n",
      "Epoch 753/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1047 - val_loss: 0.5406\n",
      "Epoch 754/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1036 - val_loss: 0.5416\n",
      "Epoch 755/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1052 - val_loss: 0.5404\n",
      "Epoch 756/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1055 - val_loss: 0.5408\n",
      "Epoch 757/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1033 - val_loss: 0.5401\n",
      "Epoch 758/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1039 - val_loss: 0.5392\n",
      "Epoch 759/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1048 - val_loss: 0.5390\n",
      "Epoch 760/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1052 - val_loss: 0.5387\n",
      "Epoch 761/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1033 - val_loss: 0.5403\n",
      "Epoch 762/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1034 - val_loss: 0.5396\n",
      "Epoch 763/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1052 - val_loss: 0.5395\n",
      "Epoch 764/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1031 - val_loss: 0.5404\n",
      "Epoch 765/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1055 - val_loss: 0.5402\n",
      "Epoch 766/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1039 - val_loss: 0.5393\n",
      "Epoch 767/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1036 - val_loss: 0.5383\n",
      "Epoch 768/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1042 - val_loss: 0.5381\n",
      "Epoch 769/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1029 - val_loss: 0.5384\n",
      "Epoch 770/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1050 - val_loss: 0.5403\n",
      "Epoch 771/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1047 - val_loss: 0.5391\n",
      "Epoch 772/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1036 - val_loss: 0.5414\n",
      "Epoch 773/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1028 - val_loss: 0.5400\n",
      "Epoch 774/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1036 - val_loss: 0.5403\n",
      "Epoch 775/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1056 - val_loss: 0.5409\n",
      "Epoch 776/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1049 - val_loss: 0.5399\n",
      "Epoch 777/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1050 - val_loss: 0.5411\n",
      "Epoch 778/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1034 - val_loss: 0.5395\n",
      "Epoch 779/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1037 - val_loss: 0.5407\n",
      "Epoch 780/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1050 - val_loss: 0.5395\n",
      "Epoch 781/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1041 - val_loss: 0.5394\n",
      "Epoch 782/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1027 - val_loss: 0.5396\n",
      "Epoch 783/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1025 - val_loss: 0.5396\n",
      "Epoch 784/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1031 - val_loss: 0.5401\n",
      "Epoch 785/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1036 - val_loss: 0.5403\n",
      "Epoch 786/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1025 - val_loss: 0.5389\n",
      "Epoch 787/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1033 - val_loss: 0.5393\n",
      "Epoch 788/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1039 - val_loss: 0.5383\n",
      "Epoch 789/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1034 - val_loss: 0.5403\n",
      "Epoch 790/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1027 - val_loss: 0.5403\n",
      "Epoch 791/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1030 - val_loss: 0.5399\n",
      "Epoch 792/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1053 - val_loss: 0.5405\n",
      "Epoch 793/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1040 - val_loss: 0.5398\n",
      "Epoch 794/1000\n",
      "1248/1248 [==============================] - 0s 100us/sample - loss: 0.1024 - val_loss: 0.5400\n",
      "Epoch 795/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1020 - val_loss: 0.5398\n",
      "Epoch 796/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1052 - val_loss: 0.5401\n",
      "Epoch 797/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1038 - val_loss: 0.5410\n",
      "Epoch 798/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1051 - val_loss: 0.5411\n",
      "Epoch 799/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1022 - val_loss: 0.5385\n",
      "Epoch 800/1000\n",
      " 500/1248 [===========>..................] - ETA: 0s - loss: 0.0988\n",
      "Epoch 00800: saving model to ./model/LSTM_03_check_point/cp-0800.ckpt\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.SGD object at 0x0000021ACD4FF4E0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "1248/1248 [==============================] - 0s 150us/sample - loss: 0.1042 - val_loss: 0.5395\n",
      "Epoch 801/1000\n",
      "1248/1248 [==============================] - 0s 100us/sample - loss: 0.1042 - val_loss: 0.5391\n",
      "Epoch 802/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1022 - val_loss: 0.5401\n",
      "Epoch 803/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1032 - val_loss: 0.5399\n",
      "Epoch 804/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1042 - val_loss: 0.5407\n",
      "Epoch 805/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1036 - val_loss: 0.5410\n",
      "Epoch 806/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1038 - val_loss: 0.5398\n",
      "Epoch 807/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1032 - val_loss: 0.5393\n",
      "Epoch 808/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1041 - val_loss: 0.5397\n",
      "Epoch 809/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1040 - val_loss: 0.5389\n",
      "Epoch 810/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1045 - val_loss: 0.5403\n",
      "Epoch 811/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1034 - val_loss: 0.5385\n",
      "Epoch 812/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1041 - val_loss: 0.5380\n",
      "Epoch 813/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1039 - val_loss: 0.5371\n",
      "Epoch 814/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1042 - val_loss: 0.5390\n",
      "Epoch 815/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1034 - val_loss: 0.5390\n",
      "Epoch 816/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1018 - val_loss: 0.5394\n",
      "Epoch 817/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1029 - val_loss: 0.5376\n",
      "Epoch 818/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1027 - val_loss: 0.5385\n",
      "Epoch 819/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1045 - val_loss: 0.5392\n",
      "Epoch 820/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1031 - val_loss: 0.5391\n",
      "Epoch 821/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1030 - val_loss: 0.5411\n",
      "Epoch 822/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1038 - val_loss: 0.5395\n",
      "Epoch 823/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1018 - val_loss: 0.5401\n",
      "Epoch 824/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1049 - val_loss: 0.5401\n",
      "Epoch 825/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1032 - val_loss: 0.5388\n",
      "Epoch 826/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1032 - val_loss: 0.5379\n",
      "Epoch 827/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1027 - val_loss: 0.5379\n",
      "Epoch 828/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1037 - val_loss: 0.5377\n",
      "Epoch 829/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1029 - val_loss: 0.5382\n",
      "Epoch 830/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1025 - val_loss: 0.5395\n",
      "Epoch 831/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1020 - val_loss: 0.5393\n",
      "Epoch 832/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1041 - val_loss: 0.5373\n",
      "Epoch 833/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1023 - val_loss: 0.5390\n",
      "Epoch 834/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1024 - val_loss: 0.5392\n",
      "Epoch 835/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1037 - val_loss: 0.5388\n",
      "Epoch 836/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1026 - val_loss: 0.5380\n",
      "Epoch 837/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1036 - val_loss: 0.5394\n",
      "Epoch 838/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1022 - val_loss: 0.5387\n",
      "Epoch 839/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1022 - val_loss: 0.5385\n",
      "Epoch 840/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1022 - val_loss: 0.5393\n",
      "Epoch 841/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1033 - val_loss: 0.5400\n",
      "Epoch 842/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1014 - val_loss: 0.5405\n",
      "Epoch 843/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1017 - val_loss: 0.5394\n",
      "Epoch 844/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1036 - val_loss: 0.5381\n",
      "Epoch 845/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1048 - val_loss: 0.5385\n",
      "Epoch 846/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1030 - val_loss: 0.5398\n",
      "Epoch 847/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1027 - val_loss: 0.5384\n",
      "Epoch 848/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1044 - val_loss: 0.5368\n",
      "Epoch 849/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1037 - val_loss: 0.5380\n",
      "Epoch 850/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1024 - val_loss: 0.5386\n",
      "Epoch 851/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1018 - val_loss: 0.5385\n",
      "Epoch 852/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1030 - val_loss: 0.5388\n",
      "Epoch 853/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1018 - val_loss: 0.5388\n",
      "Epoch 854/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1039 - val_loss: 0.5391\n",
      "Epoch 855/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1045 - val_loss: 0.5390\n",
      "Epoch 856/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1032 - val_loss: 0.5382\n",
      "Epoch 857/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1026 - val_loss: 0.5380\n",
      "Epoch 858/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1030 - val_loss: 0.5384\n",
      "Epoch 859/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1028 - val_loss: 0.5383\n",
      "Epoch 860/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1019 - val_loss: 0.5389\n",
      "Epoch 861/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1026 - val_loss: 0.5389\n",
      "Epoch 862/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1031 - val_loss: 0.5378\n",
      "Epoch 863/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1034 - val_loss: 0.5384\n",
      "Epoch 864/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1040 - val_loss: 0.5371\n",
      "Epoch 865/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1030 - val_loss: 0.5378\n",
      "Epoch 866/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1028 - val_loss: 0.5388\n",
      "Epoch 867/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1022 - val_loss: 0.5386\n",
      "Epoch 868/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1009 - val_loss: 0.5380\n",
      "Epoch 869/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1005 - val_loss: 0.5376\n",
      "Epoch 870/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1031 - val_loss: 0.5367\n",
      "Epoch 871/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1021 - val_loss: 0.5389\n",
      "Epoch 872/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1029 - val_loss: 0.5385\n",
      "Epoch 873/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1027 - val_loss: 0.5384\n",
      "Epoch 874/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1021 - val_loss: 0.5382\n",
      "Epoch 875/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1033 - val_loss: 0.5382\n",
      "Epoch 876/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1017 - val_loss: 0.5384\n",
      "Epoch 877/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1004 - val_loss: 0.5402\n",
      "Epoch 878/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1029 - val_loss: 0.5399\n",
      "Epoch 879/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1016 - val_loss: 0.5392\n",
      "Epoch 880/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1017 - val_loss: 0.5392\n",
      "Epoch 881/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1018 - val_loss: 0.5390\n",
      "Epoch 882/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1021 - val_loss: 0.5380\n",
      "Epoch 883/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1026 - val_loss: 0.5390\n",
      "Epoch 884/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1020 - val_loss: 0.5392\n",
      "Epoch 885/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1038 - val_loss: 0.5382\n",
      "Epoch 886/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1020 - val_loss: 0.5389\n",
      "Epoch 887/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1033 - val_loss: 0.5381\n",
      "Epoch 888/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1032 - val_loss: 0.5398\n",
      "Epoch 889/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1031 - val_loss: 0.5381\n",
      "Epoch 890/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1030 - val_loss: 0.5379\n",
      "Epoch 891/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1043 - val_loss: 0.5379\n",
      "Epoch 892/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1011 - val_loss: 0.5374\n",
      "Epoch 893/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1030 - val_loss: 0.5376\n",
      "Epoch 894/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1027 - val_loss: 0.5382\n",
      "Epoch 895/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1019 - val_loss: 0.5378\n",
      "Epoch 896/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1029 - val_loss: 0.5376\n",
      "Epoch 897/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1011 - val_loss: 0.5390\n",
      "Epoch 898/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1005 - val_loss: 0.5391\n",
      "Epoch 899/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1034 - val_loss: 0.5401\n",
      "Epoch 900/1000\n",
      " 500/1248 [===========>..................] - ETA: 0s - loss: 0.1068\n",
      "Epoch 00900: saving model to ./model/LSTM_03_check_point/cp-0900.ckpt\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.SGD object at 0x0000021ACD4FF4E0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "1248/1248 [==============================] - 0s 138us/sample - loss: 0.1030 - val_loss: 0.5402\n",
      "Epoch 901/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1013 - val_loss: 0.5396\n",
      "Epoch 902/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1025 - val_loss: 0.5383\n",
      "Epoch 903/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1023 - val_loss: 0.5371\n",
      "Epoch 904/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1016 - val_loss: 0.5379\n",
      "Epoch 905/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1015 - val_loss: 0.5378\n",
      "Epoch 906/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1039 - val_loss: 0.5369\n",
      "Epoch 907/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1030 - val_loss: 0.5367\n",
      "Epoch 908/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1027 - val_loss: 0.5365\n",
      "Epoch 909/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1025 - val_loss: 0.5377\n",
      "Epoch 910/1000\n",
      "1248/1248 [==============================] - 0s 100us/sample - loss: 0.1027 - val_loss: 0.5368\n",
      "Epoch 911/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1013 - val_loss: 0.5367\n",
      "Epoch 912/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1021 - val_loss: 0.5375\n",
      "Epoch 913/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1021 - val_loss: 0.5385\n",
      "Epoch 914/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1028 - val_loss: 0.5393\n",
      "Epoch 915/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1014 - val_loss: 0.5394\n",
      "Epoch 916/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1019 - val_loss: 0.5378\n",
      "Epoch 917/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1020 - val_loss: 0.5372\n",
      "Epoch 918/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1034 - val_loss: 0.5362\n",
      "Epoch 919/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1017 - val_loss: 0.5378\n",
      "Epoch 920/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1015 - val_loss: 0.5383\n",
      "Epoch 921/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1016 - val_loss: 0.5383\n",
      "Epoch 922/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1010 - val_loss: 0.5372\n",
      "Epoch 923/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1023 - val_loss: 0.5383\n",
      "Epoch 924/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1013 - val_loss: 0.5376\n",
      "Epoch 925/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1014 - val_loss: 0.5379\n",
      "Epoch 926/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1002 - val_loss: 0.5387\n",
      "Epoch 927/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1019 - val_loss: 0.5380\n",
      "Epoch 928/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1023 - val_loss: 0.5373\n",
      "Epoch 929/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1011 - val_loss: 0.5379\n",
      "Epoch 930/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1011 - val_loss: 0.5392\n",
      "Epoch 931/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1017 - val_loss: 0.5389\n",
      "Epoch 932/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1026 - val_loss: 0.5386\n",
      "Epoch 933/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1015 - val_loss: 0.5371\n",
      "Epoch 934/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1027 - val_loss: 0.5375\n",
      "Epoch 935/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1015 - val_loss: 0.5377\n",
      "Epoch 936/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1031 - val_loss: 0.5368\n",
      "Epoch 937/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1030 - val_loss: 0.5365\n",
      "Epoch 938/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1018 - val_loss: 0.5380\n",
      "Epoch 939/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1019 - val_loss: 0.5373\n",
      "Epoch 940/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1021 - val_loss: 0.5382\n",
      "Epoch 941/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1009 - val_loss: 0.5384\n",
      "Epoch 942/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1018 - val_loss: 0.5373\n",
      "Epoch 943/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1031 - val_loss: 0.5381\n",
      "Epoch 944/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1015 - val_loss: 0.5381\n",
      "Epoch 945/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1025 - val_loss: 0.5383\n",
      "Epoch 946/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1012 - val_loss: 0.5383\n",
      "Epoch 947/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1018 - val_loss: 0.5381\n",
      "Epoch 948/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1015 - val_loss: 0.5380\n",
      "Epoch 949/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1029 - val_loss: 0.5368\n",
      "Epoch 950/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1016 - val_loss: 0.5368\n",
      "Epoch 951/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1030 - val_loss: 0.5383\n",
      "Epoch 952/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1011 - val_loss: 0.5381\n",
      "Epoch 953/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1011 - val_loss: 0.5378\n",
      "Epoch 954/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1024 - val_loss: 0.5373\n",
      "Epoch 955/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1033 - val_loss: 0.5376\n",
      "Epoch 956/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1009 - val_loss: 0.5391\n",
      "Epoch 957/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1017 - val_loss: 0.5373\n",
      "Epoch 958/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1022 - val_loss: 0.5371\n",
      "Epoch 959/1000\n",
      "1248/1248 [==============================] - 0s 100us/sample - loss: 0.1021 - val_loss: 0.5365\n",
      "Epoch 960/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1017 - val_loss: 0.5366\n",
      "Epoch 961/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1023 - val_loss: 0.5365\n",
      "Epoch 962/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1020 - val_loss: 0.5375\n",
      "Epoch 963/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1022 - val_loss: 0.5360\n",
      "Epoch 964/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1021 - val_loss: 0.5358\n",
      "Epoch 965/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1015 - val_loss: 0.5361\n",
      "Epoch 966/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1027 - val_loss: 0.5380\n",
      "Epoch 967/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1013 - val_loss: 0.5381\n",
      "Epoch 968/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1020 - val_loss: 0.5358\n",
      "Epoch 969/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1020 - val_loss: 0.5356\n",
      "Epoch 970/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1028 - val_loss: 0.5365\n",
      "Epoch 971/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1019 - val_loss: 0.5379\n",
      "Epoch 972/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1027 - val_loss: 0.5376\n",
      "Epoch 973/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1004 - val_loss: 0.5369\n",
      "Epoch 974/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1036 - val_loss: 0.5362\n",
      "Epoch 975/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1018 - val_loss: 0.5357\n",
      "Epoch 976/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1024 - val_loss: 0.5373\n",
      "Epoch 977/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1018 - val_loss: 0.5376\n",
      "Epoch 978/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1016 - val_loss: 0.5385\n",
      "Epoch 979/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1014 - val_loss: 0.5375\n",
      "Epoch 980/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1005 - val_loss: 0.5362\n",
      "Epoch 981/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1007 - val_loss: 0.5371\n",
      "Epoch 982/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1006 - val_loss: 0.5363\n",
      "Epoch 983/1000\n",
      "1248/1248 [==============================] - 0s 100us/sample - loss: 0.1027 - val_loss: 0.5357\n",
      "Epoch 984/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1038 - val_loss: 0.5381\n",
      "Epoch 985/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1007 - val_loss: 0.5374\n",
      "Epoch 986/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1018 - val_loss: 0.5389\n",
      "Epoch 987/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1022 - val_loss: 0.5383\n",
      "Epoch 988/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1005 - val_loss: 0.5371\n",
      "Epoch 989/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1004 - val_loss: 0.5375\n",
      "Epoch 990/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1013 - val_loss: 0.5364\n",
      "Epoch 991/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1026 - val_loss: 0.5361\n",
      "Epoch 992/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1018 - val_loss: 0.5365\n",
      "Epoch 993/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1007 - val_loss: 0.5360\n",
      "Epoch 994/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1030 - val_loss: 0.5381\n",
      "Epoch 995/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1026 - val_loss: 0.5368\n",
      "Epoch 996/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1008 - val_loss: 0.5380\n",
      "Epoch 997/1000\n",
      "1248/1248 [==============================] - 0s 75us/sample - loss: 0.1002 - val_loss: 0.5369\n",
      "Epoch 998/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1001 - val_loss: 0.5369\n",
      "Epoch 999/1000\n",
      "1248/1248 [==============================] - 0s 88us/sample - loss: 0.1001 - val_loss: 0.5367\n",
      "Epoch 1000/1000\n",
      " 500/1248 [===========>..................] - ETA: 0s - loss: 0.1045\n",
      "Epoch 01000: saving model to ./model/LSTM_03_check_point/cp-1000.ckpt\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.SGD object at 0x0000021ACD4FF4E0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "1248/1248 [==============================] - 0s 150us/sample - loss: 0.1012 - val_loss: 0.5377\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "name = 'LSTM_03'\n",
    "checkpoint_file = './model/' + name + '_check_point/cp-{epoch:04d}.ckpt'\n",
    "try:\n",
    "    os.mkdir('./model/' + name + '_check_point/')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# get what we want\n",
    "train_input = train_x\n",
    "train_label = train_y\n",
    "test_input = test_x\n",
    "test_label = test_y\n",
    "train_label = np.squeeze(train_label, axis=1)\n",
    "test_label = np.squeeze(test_label, axis=1)\n",
    "\n",
    "# create callback function\n",
    "cp_callback = ModelCheckpoint(checkpoint_file, save_weights_only=True, verbose=1, period=period)\n",
    "\n",
    "# train the model\n",
    "train = model.fit(train_input, train_label, epochs=epochs, batch_size=batch_size, callbacks=[cp_callback], \n",
    "                  validation_data=(test_input, test_label))\n",
    "\n",
    "# save model\n",
    "model.save('./model/' + name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train.history['loss'])\n",
    "plt.plot(train.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,) [319.61041309 320.48669415 321.58558839 312.70136309 306.67996973\n",
      " 304.35984528 301.40149357 299.42348053 300.14516567 301.89968084\n",
      " 304.92084684 307.9695063  290.34068517 291.68974358 294.40762677\n",
      " 299.4737228  300.55302817 302.22012037 302.90644198 303.3491701 ]\n"
     ]
    }
   ],
   "source": [
    "# local data\n",
    "# model.load_weights('./model/LSTM_03_check_point/cp-{epoch:04d}.ckpt'.format(epoch=600))\n",
    "if local_norm_flag:\n",
    "    testing_data = open_data[-test_count-input_days:]\n",
    "    output_prices = []\n",
    "    for i in range(test_count):\n",
    "        sc = MinMaxScaler()\n",
    "        test = testing_data[i:i+input_days]\n",
    "        test = sc.getScalerData(test, offset=offset)\n",
    "        output = model.predict(np.append(np.expand_dims(test, axis=0), test_x[i:i+1, :, 1:2], axis=-1))\n",
    "        output_prices.append(sc.getInverseData(output[0][0]))\n",
    "else:\n",
    "    output = model.predict(test_input)\n",
    "    output_qute = 1 + open_sc.getInverseData(output)[-test_count:]\n",
    "    value_init = open_data[-test_count-1:-1]\n",
    "    output_prices = value_init * output_qute\n",
    "output_prices = np.asarray(output_prices)\n",
    "print(output_prices.shape, output_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualising the results\n",
    "plt.plot(test_data_t[1:, 0], color = 'red', label = 'Real Google Stock Price')\n",
    "plt.plot(output_prices, color = 'blue', label = 'Predicted Google Stock Price')\n",
    "plt.title('Prediction')\n",
    "plt.xlabel('Time(days)')\n",
    "plt.ylabel('Real')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1248,) [320.21771833 321.05380907 322.04292328 ... 796.08553921 795.77559688\n",
      " 791.97799407]\n"
     ]
    }
   ],
   "source": [
    "# local data\n",
    "# model.load_weights('./model/LSTM_03_check_point/cp-{epoch:04d}.ckpt'.format(epoch=600))\n",
    "\n",
    "train_count = len(train_data_t) - input_days\n",
    "if local_norm_flag:\n",
    "    testing_data = open_data[:-test_count]\n",
    "    output_prices = []\n",
    "    for i in range(train_count):\n",
    "        sc = MinMaxScaler()\n",
    "        test = testing_data[i:i+input_days]\n",
    "        test = sc.getScalerData(test, offset=offset)\n",
    "        output = model.predict(np.append(np.expand_dims(test, axis=0), train_x[i:i+1, :, 1:2], axis=-1))\n",
    "        output_prices.append(sc.getInverseData(output[0][0]))\n",
    "output_prices = np.asarray(output_prices)\n",
    "print(output_prices.shape, output_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualising the results\n",
    "if local_norm_flag:\n",
    "    plt.plot(train_data_t[input_days:, 0], color = 'red', label = 'Real Google Stock Price')\n",
    "    plt.plot(output_prices, color = 'blue', label = 'Predicted Google Stock Price')\n",
    "    plt.title('Prediction')\n",
    "    plt.xlabel('Time(days)')\n",
    "    plt.ylabel('Real')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dlib]",
   "language": "python",
   "name": "conda-env-dlib-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
